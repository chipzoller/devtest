<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s on Neon Mirrors</title>
    <link>https://neonmirrors.net/tags/k8s/</link>
    <description>Recent content in k8s on Neon Mirrors</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>Copyright © 2020, Chip Zoller and the Hugo Authors; all rights reserved.</copyright>
    <lastBuildDate>Sat, 11 Apr 2020 08:24:25 -0400</lastBuildDate>
    
	<atom:link href="https://neonmirrors.net/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Getting Started with Tanzu Kubernetes Grid</title>
      <link>https://neonmirrors.net/post/2020-04/getting-started-with-tkg/</link>
      <pubDate>Sat, 11 Apr 2020 08:24:25 -0400</pubDate>
      
      <guid>https://neonmirrors.net/post/2020-04/getting-started-with-tkg/</guid>
      <description>After VMworld US 2019 when the Tanzu portfolio was announced, there was naturally a lot of excitement for what VMware was doing around modern application development. Fast forward a couple months when more things were announced, names were changed, and Pivotal became part of VMware, and what Tanzu actually is and what the product sets are has been wildly confusing to most. One of those offerings called Tanzu Kubernetes Grid (TKG) has now been released, and so in this article I want to explain what this offering is, what it promises, and, lastly, how to get started with the CLI tool by putting rubber to the road.</description>
    </item>
    
    <item>
      <title>Behind the Scenes with Cluster API Provider vSphere</title>
      <link>https://neonmirrors.net/post/2020-04/capv-overview/</link>
      <pubDate>Wed, 08 Apr 2020 10:41:47 -0400</pubDate>
      
      <guid>https://neonmirrors.net/post/2020-04/capv-overview/</guid>
      <description>The verdict is in and Kubernetes has proven a great success in its ability to deploy and manage containerized applications. And in that regard, it is now seen as the de facto platform for running cloud-native applications. A somewhat more recent project has emerged which seeks to apply that same system of desired state reconciliation to Kubernetes clusters themselves, and this is called the Cluster API (CAPI) project. Cluster API has already been written about numerous times (this blog is a good overview; so is this for more of a technical primer) and so I won&amp;rsquo;t rehash its merits here.</description>
    </item>
    
    <item>
      <title>Why Kubernetes on Virtual Machines?</title>
      <link>https://neonmirrors.net/post/2020-01/why-k8s-on-vms/</link>
      <pubDate>Fri, 17 Jan 2020 18:35:21 -0500</pubDate>
      
      <guid>https://neonmirrors.net/post/2020-01/why-k8s-on-vms/</guid>
      <description>One of the arguments or debates (depending on how generous you are) that continues to rage in the world of Kubernetes is the old &amp;ldquo;bare metal versus virtual machines&amp;rdquo; subject. Some people seem to be all in on bare metal while you have those staunch opponents (as well as some hilarious memes) who advise not even attempting it.
The reality, of course, is an &amp;ldquo;it depends&amp;rdquo; moment, with pros and cons on each side.</description>
    </item>
    
    <item>
      <title>Authentication and Authorization in Kubernetes</title>
      <link>https://neonmirrors.net/post/2019-10/authentication-and-authorization-in-k8s/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://neonmirrors.net/post/2019-10/authentication-and-authorization-in-k8s/</guid>
      <description>Kubernetes is designed to be secured by default, and many of the built-in technologies and concepts are designed to ensure that is the case. Often times, the first exposure users will have to Kubernetes security will be to authentication and authorization: simply getting inside the cluster and being permitted to do something. In this blog post, I want to focus on human user/operator access to Kubernetes and the mechanisms at your disposal to ensure you get it right the first try.</description>
    </item>
    
    <item>
      <title>pks-rancher-reg: Automated PKS Cluster Registration For Rancher</title>
      <link>https://neonmirrors.net/post/2019-10/pks-rancher-reg/</link>
      <pubDate>Tue, 15 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://neonmirrors.net/post/2019-10/pks-rancher-reg/</guid>
      <description>My previous blog showed a method that can be used to stand up Rancher Server in HA on VMware PKS, and I’m going to build on it in this one. Having a Rancher Server environment is great for managing all sorts of Kubernetes clusters from the edge to those hosted in cloud providers, but we can also use it to manage VMware PKS clusters as well. That’s really no secret. However, since PKS focuses on a high degree of difficult automation to produce ready-to-run Kubernetes clusters at the end of the day, wouldn’t it be nice to extend that to making them ready-to-manage as well?</description>
    </item>
    
    <item>
      <title>Rancher HA on Enterprise PKS</title>
      <link>https://neonmirrors.net/post/2019-09/rancher-ha-on-pks/</link>
      <pubDate>Wed, 25 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://neonmirrors.net/post/2019-09/rancher-ha-on-pks/</guid>
      <description>Rancher is a container orchestration and management tool that has been around for several years at this point and performs a variety of different functions. In more recent days, it has been refactored to completely adopt Kubernetes. In this blog, I am going to focus on how to build an enterprise-grade, highly-available, and secure installation of Rancher Server on top of VMware Enterprise PKS. I’ll respond to the burning question of ‘why Rancher on PKS in the first place?</description>
    </item>
    
    <item>
      <title>HTTPS Ingress with Enterprise PKS</title>
      <link>https://neonmirrors.net/post/2019-09/https-ingress-with-pks/</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://neonmirrors.net/post/2019-09/https-ingress-with-pks/</guid>
      <description>Kubernetes is an awesome technology, in my humble opinion, and one of the best ways to adopt it and begin to use it in a production-worthy manner is with VMware’s Enterprise PKS. By using Enterprise PKS, you get some truly great additional value by leveraging NSX-T for the networking and security components. One of those things that comes in so handy is the ingress controller capabilities of NSX-T in which you don’t need to go roll your own.</description>
    </item>
    
    <item>
      <title>pks-dns: Automated DNS registration for PKS Clusters</title>
      <link>https://neonmirrors.net/post/2019-08/pks-dns/</link>
      <pubDate>Fri, 09 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://neonmirrors.net/post/2019-08/pks-dns/</guid>
      <description>VMware’s Enterprise PKS is a great solution for quickly and easily stamping out Kubernetes clusters. It really makes the process simple and repeatable while bringing additional benefits like scaling, upgrades, and auto-healing thanks to the likes of BOSH–PKS’ right-hand man. It also leverages (or can) NSX-T which provides tons of additional value. All of these pieces come together to form what is often touted as a “turnkey solution”. However, ever since I first started working with the product, something has always bugged me and remained in the back of my mind like a chime ringing in the distance.</description>
    </item>
    
    <item>
      <title>Optimize-VMwarePKS: A PowerShell script for all your VMware PKS deployment needs</title>
      <link>https://neonmirrors.net/post/2019-04/optimize-vmwarepks/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://neonmirrors.net/post/2019-04/optimize-vmwarepks/</guid>
      <description>Ever since VMware PKS (now called Enterprise PKS) came onto the market over a year ago, it’s been a big hit. With it, you get upstream Kubernetes, NSX-T, an enterprise-class container registry, automation of the entire K8s cluster creation process, and lots more all on top of the de facto private cloud platform of vSphere. It’s truly becoming the way organizations are standardizing on K8s cluster instantiation, upgrade, and management on-premises.</description>
    </item>
    
  </channel>
</rss>